{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"/Users/delione/.vscode/extensions/ms-python.python-2025.2.0-darwin-arm64/python_files/python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "ModuleNotFoundError: No module named 'pandas'\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# ==============================\n",
    "# Step 1: Load Data and Build Graph Structures\n",
    "# ==============================\n",
    "\n",
    "# Load event data from features.csv\n",
    "df = pd.read_csv(\"features.csv\")\n",
    "\n",
    "# Encode actor_name (users) as unique numerical identifiers\n",
    "user_mapping = {name: idx for idx, name in enumerate(df['actor_name'].unique())}\n",
    "df['user_id'] = df['actor_name'].map(user_mapping)\n",
    "\n",
    "# Use the repo_index from the CSV (already unique) and build a mapping to row indices\n",
    "unique_repos = df['repo_index'].unique()\n",
    "repo_mapping = {repo: i for i, repo in enumerate(unique_repos)}\n",
    "\n",
    "# Counts\n",
    "num_users = len(user_mapping)\n",
    "num_repos = len(unique_repos)\n",
    "\n",
    "# Get unique event types\n",
    "event_types = df['event_type'].unique()\n",
    "num_event_types = len(event_types)\n",
    "\n",
    "# 1a. Build binary repo-user bipartite adjacency matrices (one per event type)\n",
    "repo_user_adj_matrices = {event: np.zeros((num_repos, num_users)) for event in event_types}\n",
    "for _, row in df.iterrows():\n",
    "    repo_id = repo_mapping[row['repo_index']]\n",
    "    user_id = row['user_id']\n",
    "    event_type = row['event_type']\n",
    "    # Binary: set to 1 if any interaction exists\n",
    "    repo_user_adj_matrices[event_type][repo_id, user_id] = 1\n",
    "\n",
    "# Convert each to a sparse CSR matrix for efficiency\n",
    "repo_user_sparse_matrices = {event: csr_matrix(matrix) for event, matrix in repo_user_adj_matrices.items()}\n",
    "\n",
    "# 1b. Build the repository feature matrix (sum of event counts per repo)\n",
    "repo_feature_matrix = np.zeros((num_repos, num_event_types))\n",
    "for _, row in df.iterrows():\n",
    "    repo_id = repo_mapping[row['repo_index']]\n",
    "    event_type = row['event_type']\n",
    "    # Find column index for this event type\n",
    "    event_idx = np.where(event_types == event_type)[0][0]\n",
    "    repo_feature_matrix[repo_id, event_idx] += row['num_event']\n",
    "\n",
    "# Global z-score normalization (all values together)\n",
    "global_mean = repo_feature_matrix.mean()\n",
    "global_std = repo_feature_matrix.std()\n",
    "repo_feature_matrix = (repo_feature_matrix - global_mean) / global_std\n",
    "\n",
    "# Convert to torch tensor\n",
    "repo_features = torch.tensor(repo_feature_matrix, dtype=torch.float)\n",
    "\n",
    "# Initialize user features as zeros (same number of features as repo features)\n",
    "user_features = torch.zeros((num_users, num_event_types), dtype=torch.float)\n",
    "\n",
    "# 1c. Build the global (combined) edge index for the bipartite graph\n",
    "# We combine nodes so that nodes 0..num_repos-1 are repos and nodes num_repos..num_repos+num_users-1 are users.\n",
    "\n",
    "edge_indices_list = []\n",
    "edge_types_list = []\n",
    "\n",
    "for event_idx, event in enumerate(event_types):\n",
    "    # Get the sparse matrix for this event type and convert to COO format\n",
    "    coo = repo_user_sparse_matrices[event].tocoo()\n",
    "    # For repo->user, the source nodes are repo indices (already in 0..num_repos-1)\n",
    "    # and the target nodes are user indices; but we must add an offset (num_repos) to user indices.\n",
    "    rows = coo.row\n",
    "    cols = coo.col + num_repos  # add offset\n",
    "    edge_index_ru = torch.tensor([rows, cols], dtype=torch.long)\n",
    "    edge_indices_list.append(edge_index_ru)\n",
    "    edge_types_list.append(torch.full((edge_index_ru.shape[1],), event_idx, dtype=torch.long))\n",
    "\n",
    "# Concatenate all repo->user edges and corresponding types\n",
    "edge_index_ru = torch.cat(edge_indices_list, dim=1)\n",
    "edge_type_ru = torch.cat(edge_types_list, dim=0)\n",
    "\n",
    "# For the reverse edges (user->repo), simply flip the edge_index_ru:\n",
    "edge_index_ur = edge_index_ru[[1, 0]]  # Now, source is user (global index), target is repo.\n",
    "edge_type_ur = edge_type_ru.clone()  # Same relation type\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Load Labels and Prepare Train/Test Masks\n",
    "# ==============================\n",
    "\n",
    "# Load labels from label.csv\n",
    "df_labels = pd.read_csv(\"label.csv\")\n",
    "\n",
    "# Initialize label array for all repos with -1 (for missing labels)\n",
    "repo_labels = np.full(num_repos, -1)\n",
    "for _, row in df_labels.iterrows():\n",
    "    repo_id = row['repo_index']\n",
    "    if repo_id in repo_mapping:\n",
    "        repo_labels[repo_mapping[repo_id]] = row['label']\n",
    "repo_labels = torch.tensor(repo_labels, dtype=torch.long)\n",
    "\n",
    "# Create train/test split (only for repos with valid labels, i.e., label != -1)\n",
    "valid_idx = (repo_labels != -1).nonzero(as_tuple=True)[0]\n",
    "train_idx, test_idx = train_test_split(valid_idx.cpu().numpy(), test_size=0.3, random_state=42)\n",
    "train_mask = torch.zeros(num_repos, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_repos, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# ==============================\n",
    "# Step 3: Combine Node Features into a Single Tensor\n",
    "# ==============================\n",
    "# Combined nodes: repos first, then users.\n",
    "x = torch.cat([repo_features, user_features], dim=0)  # shape: (num_repos+num_users, num_event_types)\n",
    "#x = repo_features\n",
    "# ==============================\n",
    "# Step 4: Define the Bipartite R-GCN Model\n",
    "# ==============================\n",
    "\n",
    "class BipartiteRGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        # Initial projection from input to hidden dimensions.\n",
    "        self.mlp = nn.Linear(in_channels, hidden_channels)\n",
    "        # First layer: aggregate from repos to users (repo->user edges)\n",
    "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
    "        # Second layer: aggregate from users to repos (user->repo edges)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
    "        # Classifier for repository embeddings.\n",
    "        self.classifier = nn.Linear(hidden_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout layer.\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "    def forward(self, x, edge_index_ru, edge_index_ur, edge_type_ru, edge_type_ur):\n",
    "        # Initial projection and activation.\n",
    "        x = self.mlp(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Propagate information from repos to users.\n",
    "        x = self.conv1(x, edge_index_ru, edge_type_ru)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Propagate information from users to repos.\n",
    "        x = self.conv2(x, edge_index_ur, edge_type_ur)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Extract repository embeddings (first num_repos rows)\n",
    "        repo_emb = x[:num_repos]\n",
    "        out = self.classifier(repo_emb)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = BipartiteRGCN(in_channels=num_event_types, hidden_channels=128, out_channels=2, num_relations=num_event_types)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ==============================\n",
    "# Step 5: Training and Testing Functions\n",
    "# ==============================\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index_ru, edge_index_ur, edge_type_ru, edge_type_ur)\n",
    "    loss = criterion(out[train_mask], repo_labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index_ru, edge_index_ur, edge_type_ru, edge_type_ur)\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = accuracy_score(repo_labels[test_mask].cpu(), pred[test_mask].cpu())\n",
    "    return acc\n",
    "\n",
    "# ==============================\n",
    "# Step 6: Training Loop\n",
    "# ==============================\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        acc = test()\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Test Accuracy = {acc:.4f}\")\n",
    "        \n",
    "final_acc = test()\n",
    "print(f\"Final Test Accuracy: {final_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
